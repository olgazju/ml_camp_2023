{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM Networks\n",
    "LSTM (Long Short-Term Memory) networks are a type of recurrent neural network (RNN) capable of learning order dependence in sequence prediction problems. Unlike standard feedforward neural networks, LSTM has feedback connections that make it capable of processing not only single data points but also entire sequences of data.\n",
    "\n",
    "### Predictors and Labels in LSTM\n",
    "In the context of text generation using LSTMs:\n",
    "\n",
    "- **Predictors** are the input sequences to the model. These are portions of text (or tokens) that the model will use to make its predictions. The model learns to predict the next token in a sequence based on these inputs.\n",
    "\n",
    "- **Labels** are the actual outcomes the model is trying to predict. In text generation, a label is typically the next token in the sequence that follows the input sequence.\n",
    "\n",
    "### Example:\n",
    "Imagine you have a sentence: \"The quick brown fox jumps\"\n",
    "\n",
    "If you break this sentence into sequences of words for training an LSTM, your predictors (input sequences) and labels might look like this:\n",
    "\n",
    "- Predictor: \"The\", Label: \"quick\"\n",
    "- Predictor: \"The quick\", Label: \"brown\"\n",
    "- Predictor: \"The quick brown\", Label: \"fox\"\n",
    "- Predictor: \"The quick brown fox\", Label: \"jumps\"\n",
    "\n",
    "### In the Code:\n",
    "- **Tokenization**: The text data (lyrics, in your case) is converted into tokens. Each unique word is given a unique integer (token).\n",
    "\n",
    "- **Sequence Creation**: Sequences of tokens are created. Each sequence is a set of tokens (words) from the text.\n",
    "\n",
    "- **Padding**: Sequences are padded to have the same length for training the LSTM.\n",
    "\n",
    "- **Predictors**: The predictors are all the tokens in a sequence except the last one.\n",
    "\n",
    "- **Label**: The label is the last token in the sequence. This is what the model tries to predict.\n",
    "\n",
    "In the model training process, the LSTM network learns to predict the label based on the predictors. For example, given the sequence of words \"The quick brown\", it learns to predict the next word \"fox\". This training process involves adjusting the neural network's weights through backpropagation based on the error between the predicted word and the actual next word in the sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/taylor_swift_lyrics.csv', encoding = \"latin1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-12-16 23:29:03.192992: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lyric_clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>he said the way my blue eyes shined</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>put those georgia stars to shame that night</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>i said that is a lie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>just a boy in a chevy truck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that had a tendency of gettin' stuck</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4857</th>\n",
       "      <td>hold on to the memories they will hold on to you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4858</th>\n",
       "      <td>please do not ever become a stranger</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4859</th>\n",
       "      <td>hold on to the memories they will hold on to you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4860</th>\n",
       "      <td>whose laugh i could recognize anywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4861</th>\n",
       "      <td>i will hold on to you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4862 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           lyric_clean\n",
       "0                  he said the way my blue eyes shined\n",
       "1          put those georgia stars to shame that night\n",
       "2                                 i said that is a lie\n",
       "3                          just a boy in a chevy truck\n",
       "4                 that had a tendency of gettin' stuck\n",
       "...                                                ...\n",
       "4857  hold on to the memories they will hold on to you\n",
       "4858              please do not ever become a stranger\n",
       "4859  hold on to the memories they will hold on to you\n",
       "4860            whose laugh i could recognize anywhere\n",
       "4861                             i will hold on to you\n",
       "\n",
       "[4862 rows x 1 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/cleaned.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_lyrics = ' '.join(df['lyric_clean'].dropna())\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts([combined_lyrics])\n",
    "total_words = len(tokenizer.word_index) + 1\n",
    "input_sequences = []\n",
    "for line in df['lyric_clean'].dropna():\n",
    "    token_list = tokenizer.texts_to_sequences([line])[0]\n",
    "    for i in range(1, len(token_list)):\n",
    "        n_gram_sequence = token_list[:i+1]\n",
    "        input_sequences.append(n_gram_sequence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_sequence_len = max(len(x) for x in input_sequences)\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n",
    "predictors, label = input_sequences[:,:-1], input_sequences[:,-1]\n",
    "label = to_categorical(label, num_classes=total_words)\n",
    "\n",
    "# Splitting Data\n",
    "X_train, X_val, y_train, y_val = train_test_split(predictors, label, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building the Model\n",
    "model = Sequential()\n",
    "model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n",
    "model.add(LSTM(150, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(total_words, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "# Perplexity Calculation\n",
    "class Perplexity(Callback):\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        cross_entropy = logs.get('loss')\n",
    "        perplexity = np.exp(cross_entropy)\n",
    "        print(f' - perplexity: {perplexity}')\n",
    "\n",
    "# Model Training\n",
    "history = model.fit(X_train, y_train, epochs=100, verbose=1, validation_data=(X_val, y_val), callbacks=[Perplexity()])\n",
    "\n",
    "# Visualizing Training and Validation Metrics\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_camp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
